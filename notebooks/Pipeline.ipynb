{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Joel Piper 2016-09-17 20:10:01 \n",
      "\n",
      "CPython 2.7.12\n",
      "IPython 4.2.0\n",
      "\n",
      "numpy 1.11.1\n",
      "pandas 0.18.1\n",
      "nltk 3.2.1\n",
      "sklearn 0.17.1\n",
      "gensim 0.13.2\n",
      "Git hash: 2e718645ec0e62dd529a4b2784c93c884eff7694\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a \"Joel Piper\" -d -t -v -p numpy,pandas,nltk,sklearn,gensim -g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Lemmatize the Word List in Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "con = psycopg2.connect(dbname='bills_db', user='Joel')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return first 1000 us bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# query:\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM us_bills\n",
    "LIMIT 1000;\n",
    "\"\"\"\n",
    "us_bills = pd.read_sql_query(sql_query, con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now get the subjects for those 1000 bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# query:\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM bill_subject\n",
    "WHERE bill_num IN (' {0} ');\n",
    "\"\"\"\n",
    "\n",
    "revised = sql_query.format(\"','\".join(us_bills['bill_num']))\n",
    "subjects = pd.read_sql_query(revised, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bill_subset = us_bills.ix[0:1000,['bill_name','bill_text']]\n",
    "bill_tuples = tuples = [tuple(x) for x in bill_subset.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import word_tokenize          \n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#######\n",
    "# Use a lemmatizer rather than just a stemmer\n",
    "#stemmer = PorterStemmer()\n",
    "#def stem_tokens(tokens, stemmer):\n",
    "#    stemmed = []\n",
    "#    for item in tokens:\n",
    "#        stemmed.append(stemmer.stem(item))\n",
    "#    return stemmed\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_tokens(tokens, lemma):\n",
    "    lemmatized = []\n",
    "    for item in tokens:\n",
    "        lemmatized.append(lemma.lemmatize(item))\n",
    "    return lemmatized\n",
    "\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    text = \"\".join([ch for ch in text if ch not in string.digits])\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = lemmatize_tokens(tokens, wordnet_lemmatizer)\n",
    "    return lemmas\n",
    "\n",
    "def my_preproc_text(bill_tuple):\n",
    "    text = bill_tuple[1].lower()\n",
    "    revised = \" \".join([t for t in text.split() if len(t) > 3])\n",
    "    return revised\n",
    "\n",
    "def my_preproc_title(bill_tuple):\n",
    "    title = bill_tuple[0].lower()\n",
    "    revised = \" \".join([t for t in title.split() if len(t) > 3])\n",
    "    return revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "tf_text = CountVectorizer(stop_words='english', token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=tokenize,\n",
    "                          preprocessor=my_preproc_text, ngram_range=(1,2), min_df=10, max_df=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_title = CountVectorizer(stop_words='english', token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=tokenize,\n",
    "                           preprocessor=my_preproc_title, ngram_range=(1,3), min_df=10, max_df=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the TF/IDF and LDA Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_text = TfidfVectorizer(stop_words='english', token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=tokenize,\n",
    "                             preprocessor=my_preproc_text, ngram_range=(1,2), min_df=10, max_df=0.4)\n",
    "tfidf_title = TfidfVectorizer(stop_words='english', token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=tokenize,\n",
    "                             preprocessor=my_preproc_title, ngram_range=(1,3), min_df=10, max_df=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_text = LatentDirichletAllocation(n_topics=100, max_iter=5, learning_method='online', learning_offset=50., \n",
    "                                     random_state=0)\n",
    "lda_title = LatentDirichletAllocation(n_topics=10, max_iter=5, learning_method='online', learning_offset=50., \n",
    "                                     random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "n_top_words = 20\n",
    "tf_feature_names = tf_title.get_feature_names()\n",
    "print_top_words(lda_title, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create the logistic regression model using gridcv from scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "health_bills = subjects[subjects['subject'] == 'Health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "us_bills['health'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "us_bills.ix[us_bills['bill_num'].isin(health_bills['bill_num']),'health'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_text_model = Pipeline(steps=[('tf_text', tf_text), ('lda_text', lda_text)])\n",
    "lda_title_model = Pipeline(steps=[('tf_title', tf_title), ('lda_title', lda_title)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_features = FeatureUnion([(\"tfidf_text\", tfidf_text), (\"lda_text_model\", lda_text_model), \n",
    "                                  (\"tfidf_title\", tfidf_title), (\"lda_title_model\", lda_title_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression(C=1e9, penalty='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('features', combined_features), ('logistic', logistic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = dict(features__lda_text_model__lda_text__n_topics=[100],\n",
    "                  features__lda_title_model__lda_title__n_topics=[10],\n",
    "                  features__tfidf_text__max_features=[None, 100],\n",
    "                  features__tfidf_title__max_features=[None],\n",
    "                  logistic__C=[0.1, 1, 10, 1e9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, scoring='roc_auc', verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_search.fit(bill_tuples, us_bills['health'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For visualization and reporting interest only, leave out 10% to plot a ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "fig_dir = '/Users/Joel/Desktop/Insight/data/'\n",
    "def make_roc_curve(pipeline, X, y, train_frac, subject, fig_dir):\n",
    "    X_train, X_test, y_train, y_test = train_test_split( X, y, train_size=train_frac, random_state=1, stratify=y)\n",
    "    grid_search = GridSearchCV(pipeline, {}, scoring='roc_auc', verbose=10)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    y_pred_class = grid_search.predict(X_test)\n",
    "    y_pred_prob = grid_search.predict_proba(X_test)[:, 1]\n",
    "    print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "    print(metrics.confusion_matrix(y_test, y_pred_class))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    # method I: plt\n",
    "\n",
    "    plt.title(subject + '\\nReceiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    plt.savefig(fig_dir+'/roc_curve_'+subject.lower()+'.png')\n",
    "    results_save = (grid_search, X_test, y_test)\n",
    "    pickle.dump(results_save, open(fig_dir+'/plot_info_nb.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search, X_test, y_test = pickle.load(open(fig_dir+'/plot_info.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_roc_curve(pipe_nb, bill_tuples, us_bills['health'], 0.9, 'Health', '/Users/Joel/Desktop/Insight/data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "pipe_svc = Pipeline(steps=[('features', combined_features), ('svc', svc)])\n",
    "param_grid_svc = dict(features__lda_text_model__lda_text__n_topics=[100],\n",
    "                  features__lda_title_model__lda_title__n_topics=[10],\n",
    "                  features__tfidf_text__max_features=[None],\n",
    "                  features__tfidf_title__max_features=[None],\n",
    "                  svc__C=[1])\n",
    "grid_search_svc = GridSearchCV(pipe_svc, param_grid=param_grid_svc, scoring='roc_auc', verbose=10)\n",
    "grid_search_svc.fit(bill_tuples, us_bills['health'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid_search_svc.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "pipe_nb = Pipeline(steps=[('features', combined_features), ('nb', nb)])\n",
    "param_grid_nb = dict(features__lda_text_model__lda_text__n_topics=[100],\n",
    "                  features__lda_title_model__lda_title__n_topics=[10],\n",
    "                  features__tfidf_text__max_features=[None],\n",
    "                  features__tfidf_title__max_features=[None],\n",
    "                  nb__alpha=[1])\n",
    "grid_search_nb = GridSearchCV(pipe_nb, param_grid=param_grid_nb, scoring='roc_auc', verbose=10)\n",
    "grid_search_nb.fit(bill_tuples, us_bills['health'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [insight]",
   "language": "python",
   "name": "Python [insight]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
